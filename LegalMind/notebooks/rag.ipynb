{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ceb8d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BNS ‚Üí data/bns.json (450 chunks)\n",
      "‚úÖ BNSS ‚Üí data/bnss.json (1176 chunks)\n",
      "‚úÖ BSA ‚Üí data/bsa.json (164 chunks)\n",
      "‚úÖ CrPC ‚Üí data/crpc.json (1946 chunks)\n",
      "‚úÖ IEA ‚Üí data/iea.json (467 chunks)\n",
      "‚úÖ IPC ‚Üí data/ipc.json (1271 chunks)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import os # Make sure this is at the top of your file\n",
    "\n",
    "\n",
    "MAX_CHUNK_LENGTH = 2000\n",
    "OVERLAP = 100\n",
    "\n",
    "chapter_pattern = re.compile(r'^CHAPTER\\s+[IVXLCDM]+\\s+.*', re.IGNORECASE)\n",
    "section_pattern = re.compile(r'^\\s*(\\d{1,3}[A-Z]?)\\.\\s+(.+)$')\n",
    "subsection_pattern = re.compile(r'^\\s*\\(\\d+\\)|^\\s*[a-z]\\)')\n",
    "amendment_note_pattern = re.compile(r'^(Subs\\.|Ins\\.|Omitted|Amend\\.|Rep\\.)', re.IGNORECASE)\n",
    "\n",
    "def split_with_overlap(text: str, max_len: int = MAX_CHUNK_LENGTH, overlap: int = OVERLAP) -> List[str]:\n",
    "    if len(text) <= max_len:\n",
    "        return [text]\n",
    "    parts = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_len, len(text))\n",
    "        parts.append(text[start:end])\n",
    "        if end == len(text):\n",
    "            break\n",
    "        start = end - overlap if end - overlap > start else end\n",
    "    return parts\n",
    "\n",
    "\n",
    "def split_into_subsections(section_text: str) -> List[str]:\n",
    "    \"\"\"Split section into subsections if subsection markers exist.\"\"\"\n",
    "    lines = section_text.split(\"\\n\")\n",
    "    subsections = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        if subsection_pattern.match(line) and current:\n",
    "            subsections.append(\"\\n\".join(current).strip())\n",
    "            current = [line]\n",
    "        else:\n",
    "            current.append(line)\n",
    "    if current:\n",
    "        subsections.append(\"\\n\".join(current).strip())\n",
    "    return subsections\n",
    "\n",
    "def create_law_chunks(file_path: str, law_name: str, doc_id: str) -> List[Dict]:\n",
    "    chunks = []\n",
    "    current_section_number = None\n",
    "    current_section_title = None\n",
    "    current_chapter_heading = None\n",
    "    current_body = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw_line in f:\n",
    "            line = raw_line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if chapter_pattern.match(line):\n",
    "                current_chapter_heading = line\n",
    "                continue\n",
    "\n",
    "            section_match = section_pattern.match(line)\n",
    "            if section_match and not amendment_note_pattern.match(section_match.group(2)):\n",
    "                # save previous section\n",
    "                if current_section_number and current_body:\n",
    "                    section_text = \"\\n\".join(current_body)\n",
    "                    subsections = split_into_subsections(section_text)\n",
    "                    for sub_idx, sub in enumerate(subsections):\n",
    "                        split_parts = split_with_overlap(sub)\n",
    "                        for idx, part in enumerate(split_parts):\n",
    "                            chunk_id = f\"{doc_id}_{current_section_number}_{sub_idx}_{idx}\"\n",
    "                            chunks.append({\n",
    "                                \"chunk_id\": chunk_id,\n",
    "                                \"doc_id\": doc_id,\n",
    "                                \"text\": part,\n",
    "                                \"metadata\": {\n",
    "                                    \"chapter\": current_chapter_heading,\n",
    "                                    \"section_number\": current_section_number,\n",
    "                                    \"section_title\": current_section_title,\n",
    "                                    \"law_name\": law_name\n",
    "                                }\n",
    "                            })\n",
    "\n",
    "                # start new section\n",
    "                current_section_number = section_match.group(1).strip()\n",
    "                current_section_title = section_match.group(2).strip()\n",
    "                current_body = [line]\n",
    "            else:\n",
    "                if current_section_number:\n",
    "                    current_body.append(line)\n",
    "\n",
    "    # save last section\n",
    "    if current_section_number and current_body:\n",
    "        section_text = \"\\n\".join(current_body)\n",
    "        subsections = split_into_subsections(section_text)\n",
    "        for sub_idx, sub in enumerate(subsections):\n",
    "            split_parts = split_with_overlap(sub)\n",
    "            for idx, part in enumerate(split_parts):\n",
    "                chunk_id = f\"{doc_id}_{current_section_number}_{sub_idx}_{idx}\"\n",
    "                chunks.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"text\": part,\n",
    "                    \"metadata\": {\n",
    "                        \"chapter\": current_chapter_heading,\n",
    "                        \"section_number\": current_section_number,\n",
    "                        \"section_title\": current_section_title,\n",
    "                        \"law_name\": law_name\n",
    "                    }\n",
    "                })\n",
    "\n",
    "    return chunks\n",
    "laws = [\n",
    "    {\"input_file\": \"BNS.txt\", \"law_name\": \"BNS\", \"doc_id\": \"bns_2023\"},\n",
    "    {\"input_file\": \"BNSS.txt\", \"law_name\": \"BNSS\", \"doc_id\": \"bnss_2023\"},\n",
    "    {\"input_file\": \"BSA.txt\", \"law_name\": \"BSA\", \"doc_id\": \"bsa_2023\"},\n",
    "    {\"input_file\": \"CrPC.txt\", \"law_name\": \"CrPC\", \"doc_id\": \"crpc_1973\"},\n",
    "    {\"input_file\": \"IEA.txt\", \"law_name\": \"IEA\", \"doc_id\": \"iea_1872\"},\n",
    "    {\"input_file\": \"IPC.txt\", \"law_name\": \"IPC\", \"doc_id\": \"ipc_1860\"},\n",
    "]\n",
    "# Create the 'data' directory if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "for law in laws:\n",
    "    input_path = os.path.join(\"data\", law[\"input_file\"])\n",
    "    chunks = create_law_chunks(input_path, law[\"law_name\"], law[\"doc_id\"])\n",
    "    \n",
    "    out_file = f\"{law['law_name'].lower()}.json\"\n",
    "    output_path = os.path.join(\"data\", out_file)\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ {law['law_name']} ‚Üí {output_path} ({len(chunks)} chunks)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c81b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for PDFs in: /home/prince/Desktop/llm-project/LegalMind/LegalMind/data\n",
      "Files found: ['IPC.txt', 'IEA.txt', 'BSA to IEA.txt', 'BNS_to_IPC.pdf', 'BNSS.txt', 'crpc.json', 'BNSS to CrPC.txt', 'BSA.txt', 'BSA_to_IEA.pdf', 'BNS.txt', 'bns.json', 'CrPC.txt', 'bnss.json', 'ipc.json', 'bsa.json', 'iea.json', 'BNSS_to_CrPC.pdf', 'BNS to IPC.txt']\n",
      "‚úÖ Extracted 532 rows from /home/prince/Desktop/llm-project/LegalMind/LegalMind/data/BNS_to_IPC.pdf\n",
      "‚úÖ Extracted 711 rows from /home/prince/Desktop/llm-project/LegalMind/LegalMind/data/BSA_to_IEA.pdf\n",
      "‚úÖ Extracted 1237 rows from /home/prince/Desktop/llm-project/LegalMind/LegalMind/data/BNSS_to_CrPC.pdf\n",
      "üì¶ Saved mappings to ‚Üí /home/prince/Desktop/llm-project/LegalMind/LegalMind/data/mapping_of_laws.json (1237 rows total)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pdfplumber\n",
    "\n",
    "# =========================\n",
    "# Setup paths\n",
    "# =========================\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "print(\"Looking for PDFs in:\", DATA_DIR)\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(\"Files found:\", os.listdir(DATA_DIR))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è DATA_DIR does not exist\")\n",
    "\n",
    "# =========================\n",
    "# Main processing\n",
    "# =========================\n",
    "def split_subject_summary(text):\n",
    "    \"\"\"Split text into Subject and Summary based on first period.\"\"\"\n",
    "    if not text:\n",
    "        return \"\", \"\"\n",
    "    text = text.strip()\n",
    "    # Split by first period\n",
    "    parts = text.split(\".\", 1)\n",
    "    subject = parts[0].strip()\n",
    "    summary = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "    return subject, summary\n",
    "\n",
    "def process_mappings():\n",
    "    all_chunks = []\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        print(f\"‚ö†Ô∏è No PDF files found in {DATA_DIR}\")\n",
    "        return\n",
    "\n",
    "    for file_name in pdf_files:\n",
    "        file_path = os.path.join(DATA_DIR, file_name)\n",
    "\n",
    "        try:\n",
    "            new_law_prefix, old_law_prefix = os.path.splitext(file_name)[0].split(\"_to_\")\n",
    "        except ValueError:\n",
    "            new_law_prefix = \"NEW\"\n",
    "            old_law_prefix = \"OLD\"\n",
    "\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            prev_subject = \"\"\n",
    "            prev_old_section = \"\"\n",
    "            prev_summary = \"\"\n",
    "\n",
    "            for page in pdf.pages:\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    for row in table:\n",
    "                        if not row or not row[0]:\n",
    "                            continue\n",
    "\n",
    "                        first_cell = str(row[0]).strip()\n",
    "                        if not re.match(r'^\\s*\\d+', first_cell):\n",
    "                            # continuation row ‚Üí append to previous summary\n",
    "                            extra_text = \" \".join([str(c).strip() for c in row[1:] if c]).strip()\n",
    "                            if extra_text and all_chunks:\n",
    "                                all_chunks[-1]['fields']['Summary_of_comparison'] += \" \" + extra_text\n",
    "                            continue\n",
    "\n",
    "                        new_section = first_cell\n",
    "                        remaining_text = [str(c).strip() for c in row[1:] if c and str(c).strip()]\n",
    "\n",
    "                        # Combine remaining text as one string\n",
    "                        combined_text = \" \".join(remaining_text)\n",
    "\n",
    "                        # Split into subject and summary\n",
    "                        subject, summary = split_subject_summary(combined_text)\n",
    "\n",
    "                        old_section = \"\"\n",
    "                        # Detect first numeric in remaining_text as Old Section\n",
    "                        for cell in remaining_text:\n",
    "                            if re.match(r'^\\d+', cell):\n",
    "                                old_section = cell\n",
    "                                break\n",
    "\n",
    "                        # Default old_section to prefix if not found\n",
    "                        if not old_section:\n",
    "                            old_section = \"\"\n",
    "\n",
    "                        chunk_id = f\"{file_name.replace('.pdf','')}_{new_section}\"\n",
    "\n",
    "                        chunks_to_add = {\n",
    "                            \"source_file\": file_name,\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"fields\": {\n",
    "                                \"New_Law_Section\": f\"{new_law_prefix} {new_section}\",\n",
    "                                \"Old_Law_Section\": f\"{old_law_prefix} {old_section}\" if old_section else old_law_prefix,\n",
    "                                \"Subject\": subject,\n",
    "                                \"Summary_of_comparison\": summary\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                        all_chunks.append(chunks_to_add)\n",
    "\n",
    "        print(f\"‚úÖ Extracted {len(all_chunks)} rows from {file_path}\")\n",
    "\n",
    "    output_json = os.path.join(DATA_DIR, \"mapping_of_laws.json\")\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"üì¶ Saved mappings to ‚Üí {output_json} ({len(all_chunks)} rows total)\")\n",
    "\n",
    "# =========================\n",
    "# Run\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    process_mappings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adbf2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=\"https://api.llm7.io/v1\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
